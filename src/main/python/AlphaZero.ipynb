{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import Dataset_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 9;\n",
    "CHANNELS = 2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = Dataset_pb2.DataSet()\n",
    "f = open('../resources/dataset_mcts400_forced3_all.small.bin', \"rb\")\n",
    "data_set.ParseFromString(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "train_data = []\n",
    "for data_instance in data_set.data:\n",
    "    data.append(data_instance)\n",
    "    total_games = data_instance.game_id + 1\n",
    "data = random.sample(data, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [];\n",
    "values = [];\n",
    "policies = [];\n",
    "for data_instance in data:\n",
    "    board_state = np.array(data_instance.state)\n",
    "    board_state = board_state.reshape(BOARD_SIZE, BOARD_SIZE, CHANNELS)\n",
    "    symetric_states = [board_state]\n",
    "    symetric_states = symetric_states + get_rotated(board_state)\n",
    "    symetric_states = symetric_states + get_flipped(board_state)\n",
    "    for state in symetric_states:\n",
    "        states.append(state)\n",
    "        values.append(data_instance.value)\n",
    "        #TODO flip policies\n",
    "        policies.append(data_instance.policy)\n",
    "states = np.array(states)\n",
    "values = np.array(values)\n",
    "policies = np.array(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
       "        [ 1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  1.],\n",
       "        [ 0.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.],\n",
       "        [ 1.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.],\n",
       "        [ 1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  0.,  1., 10.,  1.,  1.,  1.],\n",
       "        [ 1.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.]],\n",
       "\n",
       "       [[ 0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.],\n",
       "        [ 1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.],\n",
       "        [ 1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toNCHW(states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.],\n",
       "        [ 1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  1.],\n",
       "        [ 0.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
       "        [ 1.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
       "        [ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.],\n",
       "        [ 1.,  1.,  1., 10.,  1.,  0.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.]],\n",
       "\n",
       "       [[ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "        [ 1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.],\n",
       "        [ 0.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  1.],\n",
       "        [ 1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toNCHW(get_flipped(states[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated(board_state):\n",
    "    ret = []\n",
    "    for rot in range(1, 4):\n",
    "        rotated_board_state = np.rot90(board_state, k=rot)\n",
    "        ret.append(rotated_board_state)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_flipped(board_state):\n",
    "    ret = []\n",
    "    for axis in range(1, 3):\n",
    "        flipped_board_state = np.flip(board_state, axis=axis).copy()\n",
    "        ret.append(flipped_board_state)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNCHW(board_state):\n",
    "    return np.moveaxis(board_state, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_3d = states.reshape((len(states), BOARD_SIZE, BOARD_SIZE, CHANNELS))\n",
    "# change channels last to channels first format\n",
    "states_NCHW = np.moveaxis(states, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.035 , 0.0425, 0.0375, 0.0225, 0.    , 0.    , 0.0225, 0.    ,\n",
       "        0.035 ],\n",
       "       [0.    , 0.    , 0.    , 0.03  , 0.    , 0.    , 0.02  , 0.    ,\n",
       "        0.025 ],\n",
       "       [0.0375, 0.    , 0.    , 0.0425, 0.0325, 0.03  , 0.03  , 0.    ,\n",
       "        0.03  ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.025 , 0.    , 0.    ,\n",
       "        0.035 ],\n",
       "       [0.0425, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    ],\n",
       "       [0.035 , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.0325],\n",
       "       [0.0375, 0.0275, 0.    , 0.    , 0.    , 0.    , 0.    , 0.0225,\n",
       "        0.0375],\n",
       "       [0.    , 0.0275, 0.0325, 0.03  , 0.    , 0.    , 0.0275, 0.025 ,\n",
       "        0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.0375, 0.    , 0.0275, 0.025 , 0.    ,\n",
       "        0.    ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies.reshape(len(states), BOARD_SIZE, BOARD_SIZE)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_NCHW[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((states_3d, values)).shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 10,000 samples for validation\n",
    "# x_val = states_3d[-10000:]\n",
    "# y_val = values[-10000:]\n",
    "# x_train = states_3d[:-10000]\n",
    "# y_train = values[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNNModel(Model):\n",
    "#     def __init__(self):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.conv1 = Conv2D(32, 3, padding='same', activation='relu')\n",
    "#         self.pool1 = MaxPool2D((2,2))\n",
    "#         self.conv2 = Conv2D(64, 3, padding='same', activation='relu')\n",
    "#         self.pool2 = MaxPool2D((2,2))\n",
    "#         self.flatten = Flatten()\n",
    "#         self.d1 = Dense(512, activation='relu')\n",
    "#         self.dropout1 = Dropout(0.4)\n",
    "#         self.d2 = Dense(128, activation='relu')\n",
    "#         self.dropout2 = Dropout(0.4)\n",
    "#         self.d3 = Dense(43, activation='softmax')\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.d1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.d2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.d3(x)\n",
    "#         return x\n",
    "# model = CNNModel()\n",
    "L2_VALUE = 1e-4  # coef of l2 penalty \n",
    "\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(2, 9, 9)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Conv2D(32, activation=\"relu\", kernel_size=(3, 3),\n",
    "#                  input_shape=(BOARD_SIZE, BOARD_SIZE, CHANNELS),\n",
    "#                  data_format=\"channels_last\",\n",
    "#                  padding='same'))\n",
    "# model.add(layers.Conv2D(32, activation=\"relu\", kernel_size=(3, 3),\n",
    "#                  data_format=\"channels_last\",\n",
    "#                  padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), data_format=\"channels_last\"))\n",
    "# model.add(layers.Conv2D(64 * 2, activation=\"relu\", kernel_size=(3, 3),\n",
    "#                  data_format=\"channels_last\",\n",
    "#                  padding='same'))\n",
    "# model.add(layers.Conv2D(64 * 2, activation=\"relu\", kernel_size=(3, 3),\n",
    "#                  data_format=\"channels_last\",\n",
    "#                  padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), data_format=\"channels_last\"))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(32, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='tanh'))\n",
    "\n",
    "in_x = network = Input((BOARD_SIZE, BOARD_SIZE, CHANNELS),)\n",
    "# conv layers\n",
    "network = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "network = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "network = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "# action policy layers\n",
    "policy_net = Conv2D(filters=4, kernel_size=(1, 1), data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "policy_net = Flatten()(policy_net)\n",
    "policy_net = Dense(BOARD_SIZE*BOARD_SIZE, activation=\"softmax\", kernel_regularizer=l2(L2_VALUE), name=\"policy_output\")(policy_net)\n",
    "# state value layers\n",
    "value_net = Conv2D(filters=2, kernel_size=(1, 1), data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "value_net = Flatten()(value_net)\n",
    "value_net = Dense(64, kernel_regularizer=l2(L2_VALUE))(value_net)\n",
    "value_net = Dense(1, activation=\"tanh\", kernel_regularizer=l2(L2_VALUE), name=\"value_output\")(value_net)\n",
    "\n",
    "model = Model(in_x, [policy_net, value_net])\n",
    "\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model_mcts400_forced3_all_{epoch}.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 75083 samples, validate on 18771 samples\n",
      "Epoch 1/3\n",
      "75072/75083 [============================>.] - ETA: 0s - loss: 4.1093 - policy_output_loss: 3.5894 - value_output_loss: 0.4988\n",
      "Epoch 00001: val_loss improved from inf to 4.04460, saving model to model_mcts400_forced3_all_1.h5\n",
      "75083/75083 [==============================] - 66s 885us/sample - loss: 4.1093 - policy_output_loss: 3.5895 - value_output_loss: 0.4988 - val_loss: 4.0446 - val_policy_output_loss: 3.5332 - val_value_output_loss: 0.4942\n",
      "Epoch 2/3\n",
      "75072/75083 [============================>.] - ETA: 0s - loss: 3.9207 - policy_output_loss: 3.5413 - value_output_loss: 0.3575\n",
      "Epoch 00002: val_loss improved from 4.04460 to 3.67340, saving model to model_mcts400_forced3_all_2.h5\n",
      "75083/75083 [==============================] - 71s 943us/sample - loss: 3.9207 - policy_output_loss: 3.5413 - value_output_loss: 0.3573 - val_loss: 3.6734 - val_policy_output_loss: 3.5228 - val_value_output_loss: 0.1189\n",
      "Epoch 3/3\n",
      "75008/75083 [============================>.] - ETA: 0s - loss: 3.6155 - policy_output_loss: 3.5228 - value_output_loss: 0.0553\n",
      "Epoch 00003: val_loss improved from 3.67340 to 3.56474, saving model to model_mcts400_forced3_all_3.h5\n",
      "75083/75083 [==============================] - 66s 873us/sample - loss: 3.6156 - policy_output_loss: 3.5230 - value_output_loss: 0.0552 - val_loss: 3.5647 - val_policy_output_loss: 3.5045 - val_value_output_loss: 0.0201\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                  loss = {\n",
    "                      \"policy_output\": \"categorical_crossentropy\",\n",
    "                      \"value_output\": \"mean_squared_error\",\n",
    "                  },\n",
    "                  loss_weights = {\"policy_output\": 1.0, \"value_output\": 1.0}\n",
    "#                 metrics=['accuracy','mae']\n",
    "             )\n",
    "# validation_data=(test_images, test_labels)\n",
    "\n",
    "# Train the model by slicing the data into \"batches\"\n",
    "# of size \"batch_size\", and repeatedly iterating over\n",
    "# the entire dataset for a given number of \"epochs\"\n",
    "print('# Fit model on training data')\n",
    "history = model.fit(states, [policies, values],\n",
    "                    batch_size=64,\n",
    "                    epochs=3,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "history dict: {'loss': [4.109343210134245, 3.9207057372957506, 3.6156365291696613], 'policy_output_loss': [3.5895114, 3.5412586, 3.52299], 'value_output_loss': [0.4988098, 0.35730165, 0.05523728], 'val_loss': [4.0445963812334185, 3.673395772828995, 3.5647402849177574], 'val_policy_output_loss': [3.5331588, 3.522752, 3.5045166], 'val_value_output_loss': [0.49422672, 0.11885377, 0.020093251]}\n",
      "\n",
      "# Evaluate on test data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-3d1808166eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate the model on the test data using `evaluate`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n# Evaluate on test data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test loss, test acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "print('\\nhistory dict:', history.history)\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_val, y_val, batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (1, 81)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(states[:1])\n",
    "print('predictions shape:', predictions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_mcts400_forced3_all.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
