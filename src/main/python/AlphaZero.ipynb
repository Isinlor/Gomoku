{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thymoterdoest/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/thymoterdoest/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/thymoterdoest/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/thymoterdoest/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/thymoterdoest/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/thymoterdoest/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import Dataset_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_SIZE = 9;\n",
    "CHANNELS = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = Dataset_pb2.DataSet()\n",
    "f = open('../resources/dataset_mcts400_forced3_all.small.bin', \"rb\")\n",
    "data_set.ParseFromString(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toHWC(board_state):\n",
    "    return np.moveaxis(board_state, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_TEST_GAMES = 100\n",
    "test_data = []\n",
    "train_data = []\n",
    "for data_instance in data_set.data:\n",
    "    instance = {\n",
    "        'game_id': data_instance.game_id,\n",
    "        'state': toHWC(np.array(data_instance.state).reshape(CHANNELS, BOARD_SIZE, BOARD_SIZE)),\n",
    "        'policy': np.array(data_instance.policy).reshape(BOARD_SIZE, BOARD_SIZE),\n",
    "        'value': data_instance.value\n",
    "    }\n",
    "    if(instance['game_id']<NUMBER_OF_TEST_GAMES):\n",
    "        test_data.append(instance)\n",
    "    else:\n",
    "        train_data.append(instance)\n",
    "    total_games = instance['game_id'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    states = [];\n",
    "    values = [];\n",
    "    policies = [];\n",
    "    for instance in data:\n",
    "        state = instance['state']\n",
    "        value = instance['value']\n",
    "        policy = instance['policy']\n",
    "#         states.append(state)\n",
    "#         policies.append(policy)\n",
    "#         values.append(value)\n",
    "                  \n",
    "        for flip in range(0, 2):\n",
    "            for rot in range(0, 4):\n",
    "                flipped_state = state\n",
    "                flipped_policy = policy\n",
    "                if(flip==1):\n",
    "                    flipped_state = np.flipud(flipped_state)\n",
    "                    flipped_policy = np.flipud(flipped_policy)\n",
    "\n",
    "                rotated_state = np.rot90(flipped_state, k=rot)\n",
    "                rotated_policy = np.rot90(flipped_policy, k=rot)\n",
    "\n",
    "                states.append(rotated_state)\n",
    "                policies.append(rotated_policy)\n",
    "                values.append(value)\n",
    "                \n",
    "    states = np.array(states)\n",
    "    policies = np.array(policies).reshape(len(policies), BOARD_SIZE*BOARD_SIZE)\n",
    "    values = np.array(values)\n",
    "    \n",
    "    return states, values, policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_test, values_test, policies_test = transform_data(test_data)\n",
    "states_train, values_train, policies_train = transform_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.Dataset.from_tensor_slices((states_3d, values)).shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 335808 samples, validate on 39608 samples\n",
      "Epoch 1/3\n",
      "335616/335808 [============================>.] - ETA: 0s - loss: 0.2179\n",
      "Epoch 00001: val_loss improved from inf to 0.21229, saving model to model_mcts400_forced3_all_1.h5\n",
      "335808/335808 [==============================] - 162s 484us/sample - loss: 0.2178 - val_loss: 0.2123\n",
      "Epoch 2/3\n",
      "335616/335808 [============================>.] - ETA: 0s - loss: 0.0384\n",
      "Epoch 00002: val_loss improved from 0.21229 to 0.18577, saving model to model_mcts400_forced3_all_2.h5\n",
      "335808/335808 [==============================] - 187s 558us/sample - loss: 0.0384 - val_loss: 0.1858\n",
      "Epoch 3/3\n",
      "335616/335808 [============================>.] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00003: val_loss improved from 0.18577 to 0.18353, saving model to model_mcts400_forced3_all_3.h5\n",
      "335808/335808 [==============================] - 158s 470us/sample - loss: 0.0172 - val_loss: 0.1835\n"
     ]
    }
   ],
   "source": [
    "# class CNNModel(Model):\n",
    "#     def __init__(self):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.conv1 = Conv2D(32, 3, padding='same', activation='relu')\n",
    "#         self.pool1 = MaxPool2D((2,2))\n",
    "#         self.conv2 = Conv2D(64, 3, padding='same', activation='relu')\n",
    "#         self.pool2 = MaxPool2D((2,2))\n",
    "#         self.flatten = Flatten()\n",
    "#         self.d1 = Dense(512, activation='relu')\n",
    "#         self.dropout1 = Dropout(0.4)\n",
    "#         self.d2 = Dense(128, activation='relu')\n",
    "#         self.dropout2 = Dropout(0.4)\n",
    "#         self.d3 = Dense(43, activation='softmax')\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.d1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.d2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.d3(x)\n",
    "#         return x\n",
    "# model = CNNModel()\n",
    "L2_VALUE = 1e-4  # coef of l2 penalty \n",
    "\n",
    "model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(2, 9, 9)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Conv2D(32, activation=\"relu\", kernel_size=(3, 3),\n",
    "                 input_shape=(BOARD_SIZE, BOARD_SIZE, CHANNELS),\n",
    "                 data_format=\"channels_last\",\n",
    "                 padding='same'))\n",
    "model.add(layers.Conv2D(32, activation=\"relu\", kernel_size=(3, 3),\n",
    "                 data_format=\"channels_last\",\n",
    "                 padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2), data_format=\"channels_last\"))\n",
    "model.add(layers.Conv2D(64 * 2, activation=\"relu\", kernel_size=(3, 3),\n",
    "                 data_format=\"channels_last\",\n",
    "                 padding='same'))\n",
    "model.add(layers.Conv2D(64 * 2, activation=\"relu\", kernel_size=(3, 3),\n",
    "                 data_format=\"channels_last\",\n",
    "                 padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2), data_format=\"channels_last\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='tanh'))\n",
    "\n",
    "# in_x = network = Input((BOARD_SIZE, BOARD_SIZE, CHANNELS),)\n",
    "# # conv layers\n",
    "# network = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "# network = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "# network = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "# # action policy layers\n",
    "# policy_net = Conv2D(filters=4, kernel_size=(1, 1), data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "# policy_net = Flatten()(policy_net)\n",
    "# policy_net = Dense(BOARD_SIZE*BOARD_SIZE, activation=\"softmax\", name=\"policy_output\", kernel_regularizer=l2(L2_VALUE))(policy_net)\n",
    "# # state value layers\n",
    "# value_net = Conv2D(filters=2, kernel_size=(1, 1), data_format=\"channels_last\", activation=\"relu\", kernel_regularizer=l2(L2_VALUE))(network)\n",
    "# value_net = Flatten()(value_net)\n",
    "# value_net = Dense(64, kernel_regularizer=l2(L2_VALUE))(value_net)\n",
    "# value_net = Dense(1, activation=\"tanh\", kernel_regularizer=l2(L2_VALUE), name=\"value_output\")(value_net)\n",
    "\n",
    "# model = Model(in_x, [policy_net, value_net])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model_mcts400_forced3_all_{epoch}.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                  loss = \"mean_squared_error\",\n",
    "             )\n",
    "print('# Fit model on training data')\n",
    "\n",
    "history = model.fit(states_train, values_train,\n",
    "                    shuffle=True,\n",
    "                    batch_size=256,\n",
    "                    epochs=3,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(states_test, values_test)\n",
    "                    )\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#                   loss = {\n",
    "#                       \"policy_output\": \"categorical_crossentropy\",\n",
    "#                       \"value_output\": \"mean_squared_error\",\n",
    "#                   },\n",
    "#                   loss_weights = {\"policy_output\": 1.0, \"value_output\": 1.0},\n",
    "# #                 metrics=['accuracy','mae']\n",
    "#              )\n",
    "\n",
    "# Train the model by slicing the data into \"batches\"\n",
    "# of size \"batch_size\", and repeatedly iterating over\n",
    "# the entire dataset for a given number of \"epochs\"\n",
    "# print('# Fit model on training data')\n",
    "# history = model.fit(states_train, [policies_train, values_train],\n",
    "#                     shuffle=True,\n",
    "#                     batch_size=256,\n",
    "#                     epochs=3,\n",
    "#                     callbacks=callbacks,\n",
    "#                     validation_data=(states_test, [policies_test, values_test])\n",
    "#                     )\n",
    "\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned \"history\" object holds a record\n",
    "# of the loss values and metric values during training\n",
    "# print('\\nhistory dict:', history.history)\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(states_test, (policies_test, values_test), batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(states[:1])\n",
    "print('predictions shape:', predictions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_mcts400_forced3_all.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
